{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing production control"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will guide you through the optimization functionalities of `prodsys` to optimize the production control in a production system. With the `prodsys.control` package, we can utilize reinforcement learning, a kind of machine learning, for this task. All algorithms can be conviently used with the `prodsys.models` API. \n",
    "\n",
    "For this example, we will use again a production system which we will load from a json-file (control_configuration.json), which can be found in the examples folder of [prodsys' github page](https://github.com/sdm4fzi/prodsys/tree/main/examples/tutorials). It is the same example as in tutorial 2, but with lower arrival rates. Download it and store it in the same folder as this notebook. Load the configuration and run a simulation with the following commands:\n",
    "\n",
    "Let's start at first by loading our production system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------- Throughput -------------\n",
      "\n",
      "              Output  Throughput\n",
      "Product_type                    \n",
      "Product_1         20    0.493578\n",
      "Product_2         24    0.592294\n",
      "Product_3         26    0.641652\n",
      "------------- WIP -------------\n",
      "\n",
      "Product_type\n",
      "Product_1    1.541016\n",
      "Product_2    2.385455\n",
      "Product_3    2.682861\n",
      "Total        5.147228\n",
      "Name: WIP, dtype: float64\n",
      "\n",
      "------------- Throughput time -------------\n",
      "\n",
      "Product_type\n",
      "Product_1    122.572885\n",
      "Product_2    149.501158\n",
      "Product_3    145.277721\n",
      "Name: Throughput_time, dtype: float64\n",
      "\n",
      "------------- Resource states -------------\n",
      "\n",
      "                    time_increment  resource_time  percentage\n",
      "Resource Time_type                                           \n",
      "R1       PR            1423.549136    2879.154996   49.443296\n",
      "         SB             261.917793    2879.154996    9.097037\n",
      "         ST             833.007942    2879.154996   28.932376\n",
      "         UD             360.680125    2879.154996   12.527291\n",
      "R2       PR            2192.323458    2879.154996   76.144683\n",
      "         SB             516.230663    2879.154996   17.929937\n",
      "         UD             170.600875    2879.154996    5.925380\n",
      "R3       PR            1205.727801    2879.154996   41.877836\n",
      "         SB             835.842792    2879.154996   29.030837\n",
      "         ST             461.709190    2879.154996   16.036274\n",
      "         UD             375.875212    2879.154996   13.055053\n",
      "TR1      PR             358.133333    2879.154996   12.438835\n",
      "         SB            2503.869852    2879.154996   86.965441\n",
      "         UD              17.151810    2879.154996    0.595724\n",
      "TR2      PR             301.233333    2879.154996   10.462561\n",
      "         SB            2561.813630    2879.154996   88.977969\n",
      "         UD              16.108033    2879.154996    0.559471\n"
     ]
    }
   ],
   "source": [
    "import prodsys\n",
    "from prodsys.simulation import sim\n",
    "sim.VERBOSE = 0\n",
    "\n",
    "production_system = prodsys.adapters.JsonProductionSystemAdapter()\t\n",
    "production_system.read_data('control_configuration.json')\n",
    "\n",
    "runner = prodsys.runner.Runner(adapter=production_system)\n",
    "runner.initialize_simulation()\n",
    "runner.run(2880)\n",
    "runner.print_results()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When reviewing the performance, we see that resource R2 has the highest productivity. In order to reduce WIP and improve overall performance, we want to optimize the production control concerning R2 with Reinforcement Learning. `prodsys.control` provides a convenient API to do so, by defining interfaces for training environments for RL agents for production control task. So far, the following elementary control tasks are considered:\n",
    "\n",
    "- **Sequencing**: The agent has to decide for a resource which product to process next from a list of available products.\n",
    "- **Routing**: The agent determines for a product which resource it processes next, given a list of possible resources to perform this process.\n",
    "\n",
    "In this tutorial, we will focus on the sequencing task. The routing task is similar and can be used analogously. Note that future versions of `prodsys.control` will provide more control tasks (e.g. such as product release control) and that it is also possible to define custom control tasks that are a combination of the existing ones.\n",
    "\n",
    "## The training environment API\n",
    "\n",
    "When utilizing reinforcement learning for production control, we need to define a training environment for the RL agent. This environment is responsible for providing the agent with the current state of the production system and for executing the agent's actions. The environment is also responsible for providing the agent with a reward for each action. \n",
    "The [gymnasium](\"https://gymnasium.farama.org/) package is used as a basis for these environments to be compatible with most RL-frameworks available. For more detailed information on the gym-environment API, please read their documentation. Here, we will use [stable-baselines3](\"https://stable-baselines3.readthedocs.io/en/master/\") as RL-framework. The environments provided by `prodsys.control` are implemented as abstract base classes, specifying the methods that need to be implemented by the user for soving the associated control tasks. To realize a control environment, we need to implement a class that inherits from the abstract base classes and implements it's abstract methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import prodsys\n",
    "from prodsys.simulation import request\n",
    "from prodsys.control import sequencing_control_env\n",
    "\n",
    "class ExampleControlEnv(sequencing_control_env.AbstractSequencingControlEnv):\n",
    "    def get_observation(self) -> np.ndarray:\n",
    "        # Implement here function that returns the observation that fits to the observation space of the class instances.\n",
    "        pass\n",
    "\n",
    "    def get_info(self) -> dict:\n",
    "        # Implement here function that returns a dictionary with information about the environment.\n",
    "        pass\n",
    "\n",
    "    def get_termination_condition(self) -> bool:\n",
    "        # Implement here function that returns True if the simulation should be terminated, i.e. an episode ends.\n",
    "        pass\n",
    "\n",
    "    def get_reward(self, processed_request: request.Request, invalid_action: bool = False) -> float:\n",
    "        # Implement here function that returns the reward for the current step.\n",
    "        pass\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, only 4 functions have to implemented to start training an RL-agent. These functions define the most critical aspects when training an RL-agent, which makes these environments especially easy to experiment with different RL-agent setups and compare them. Simulation interactions are handled by the environment, so that the user can focus on the RL-agent. \n",
    "\n",
    "Especially definitions of observations and rewards are critical for the performance of the agent. The following sections will show an exemplary implementation of the environment for the sequencing task.\n",
    "\n",
    "## Example implementation of a sequencing environment\n",
    "\n",
    "In this example, we will implement the training environment for an RL-agent that determines the sequence of performed processes for the production resource R2 from the example above. \n",
    "\n",
    "For a simple optimization of performed processes, we want that the RL-agent can observe all running processes and all upcoming processes from the queue. We want to motivate the agent to sequence in a way, that the WIP is low and as few as little setups are performed, since this lower throughput. \n",
    "\n",
    "To do so, we define the observation space, to be a binary tensor of shape CxP, where C is the number of possible running processes and the number of slots in the input queue of the resource and P is the number of possible processes. This tensor shows then which slot from resource or queue is taken by which process type.\n",
    "\n",
    "The reward will be defined by a stepwise reward and a sparse reward:\n",
    "\n",
    "- **Stepwise reward**: The agent receives a reward of -1 if he selects an invalid action, 1 if he selects a valid action which requires not setup and 0 otherwise.\n",
    "- **Sparse reward**: The agent receives a reward based on the difference of queue capacity and WIP at the resource.\n",
    "\n",
    "Lastly, termination is defined by 100k minutes passed in simulation time and the info is just a placeholder.\n",
    "\n",
    "The following code shows the implementation of the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionControlEnv(sequencing_control_env.AbstractSequencingControlEnv):\n",
    "    def get_observation(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Function that utilizes the ResourceObserver of the environment class to get an array of observations of processes performed by the resource and in the queue of the resource. The observatino has a dimension CxP, where c is the capacity of resource and queue and P the number of processes.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The observation.\n",
    "        \"\"\"\n",
    "        processes_observation = self.observer.observe_processes()\n",
    "        encoded_processes = []\n",
    "        processes = self.resource.data.process_ids\n",
    "\n",
    "        for process_observation in processes_observation:\n",
    "            encoded_process = [0 for _ in range(len(processes))]\n",
    "            encoded_process[processes.index(process_observation.process)] = 1\n",
    "            encoded_processes.append(encoded_process)\n",
    "\n",
    "        encoded_process = [0 for _ in range(len(processes))]\n",
    "        encoded_processes += [encoded_process] * (\n",
    "            self.resource.data.capacity - len(processes_observation)\n",
    "        )\n",
    "\n",
    "        queue_observations = self.observer.observe_input_queue()\n",
    "        for queue_observation in queue_observations:\n",
    "            encoded_process = [0 for _ in range(len(processes))]\n",
    "            encoded_process[processes.index(queue_observation.process)] = 1\n",
    "            encoded_processes.append(encoded_process)\n",
    "\n",
    "        encoded_process = [0 for _ in range(len(processes))]\n",
    "        queue_capacity = self.resource.input_queues[0].capacity\n",
    "        encoded_processes += [encoded_process] * (\n",
    "            queue_capacity - len(queue_observations)\n",
    "        )\n",
    "\n",
    "        return np.array(encoded_processes)\n",
    "\n",
    "    def get_info(self) -> dict:\n",
    "        return {\"info\": 0}\n",
    "    \n",
    "\n",
    "    def get_termination_condition(self) -> bool:\n",
    "        return self.runner.env.now >= 100000\n",
    "    \n",
    "    def get_reward(self, processed_request: request.Request, invalid_action: bool = False) -> float:\n",
    "        if invalid_action:\n",
    "            reward = -1\n",
    "        else:\n",
    "            reward = (\n",
    "                self.resource.current_setup is None\n",
    "                or processed_request.process.process_data.ID\n",
    "                == self.resource.current_setup.process_data.ID\n",
    "            )\n",
    "        if self.step_count % 10 == 0:\n",
    "            reward += self.resource.input_queues[0].capacity - len(self.resource_controller.requests) \n",
    "        \n",
    "        return reward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we utillize the observer, which is an attribute of the environment. The observer brings handy functions to observe the current state of a resource in the simulation.\n",
    "\n",
    "In order to validate that this environment works, we will at first use just random samping as a agent and step through it. At first, we we define the observation and action space since these are required by the environment and need to fit to our get_observation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_id = \"R2\"\n",
    "resource_data = [r for r in production_system.resource_data if r.ID == resource_id][0]\n",
    "queue = [q for q in production_system.queue_data if q.ID == resource_data.input_queues[0]][0]\n",
    "shape = (queue.capacity + resource_data.capacity, len(resource_data.process_ids))\n",
    "observation_space = spaces.Box(0, 1, shape=shape, dtype=int)\n",
    "action_space = spaces.Box(0, 1, shape=(queue.capacity,), dtype=float)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can create an instance of the environment and step through it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 with a reward of -1\n",
      "Step: 1 with a reward of -1\n",
      "Step: 2 with a reward of -1\n",
      "Step: 3 with a reward of -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 4 with a reward of False\n",
      "Step: 5 with a reward of -1\n",
      "Step: 6 with a reward of -1\n",
      "Step: 7 with a reward of -1\n",
      "Step: 8 with a reward of -1\n",
      "Step: 9 with a reward of 17\n",
      "Step: 10 with a reward of False\n",
      "Step: 11 with a reward of -1\n",
      "Step: 12 with a reward of -1\n",
      "Step: 13 with a reward of -1\n",
      "Step: 14 with a reward of -1\n",
      "Step: 15 with a reward of -1\n",
      "Step: 16 with a reward of -1\n",
      "Step: 17 with a reward of -1\n",
      "Step: 18 with a reward of -1\n",
      "Step: 19 with a reward of 18\n"
     ]
    }
   ],
   "source": [
    "env = ProductionControlEnv(production_system, \"R2\", observation_space=observation_space, action_space=action_space, render_mode=\"human\")\n",
    "observation, info = env.reset(seed=42)\n",
    "for step in range(20):\n",
    "   action = env.action_space.sample()  # this is where you would insert your policy\n",
    "   observation, reward, terminated, truncated, info = env.step(action)\n",
    "   print(f\"Step: {step} with a reward of {reward}\")\n",
    "\n",
    "   if terminated or truncated:\n",
    "      observation, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we want to use a PPO RL-agent from stable-baselines3 to train the environment. We will use the default hyperparameters for the agent and train it for 20k steps. The following code shows the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to c:\\Users\\Sebas\\OneDrive\\Documents\\prodsys\\examples\\tutorials\\tensorboard_log\\sequencing\\20230625-160531\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 358  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 5    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 328         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014322889 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -28.4       |\n",
      "|    explained_variance   | -0.00496    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 62.1        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 269         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 4.49e+03   |\n",
      "|    ep_rew_mean          | 5.77e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 318        |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 19         |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01275442 |\n",
      "|    clip_fraction        | 0.112      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -28.4      |\n",
      "|    explained_variance   | 0.0307     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 78.4       |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0241    |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 290        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.49e+03    |\n",
      "|    ep_rew_mean          | 5.77e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 308         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014904976 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -28.4       |\n",
      "|    explained_variance   | 0.00739     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 68.6        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0325     |\n",
      "|    std                  | 0.999       |\n",
      "|    value_loss           | 270         |\n",
      "-----------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 4.59e+03 |\n",
      "|    ep_rew_mean          | 6.01e+03 |\n",
      "| time/                   |          |\n",
      "|    fps                  | 306      |\n",
      "|    iterations           | 5        |\n",
      "|    time_elapsed         | 33       |\n",
      "|    total_timesteps      | 10240    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.018891 |\n",
      "|    clip_fraction        | 0.191    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -28.3    |\n",
      "|    explained_variance   | 0.00564  |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 90.5     |\n",
      "|    n_updates            | 40       |\n",
      "|    policy_gradient_loss | -0.0349  |\n",
      "|    std                  | 0.998    |\n",
      "|    value_loss           | 251      |\n",
      "--------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.59e+03    |\n",
      "|    ep_rew_mean          | 6.01e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 302         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 40          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015670009 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -28.3       |\n",
      "|    explained_variance   | 0.00387     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 73.7        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.022      |\n",
      "|    std                  | 0.994       |\n",
      "|    value_loss           | 231         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 4.55e+03   |\n",
      "|    ep_rew_mean          | 6.09e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 303        |\n",
      "|    iterations           | 7          |\n",
      "|    time_elapsed         | 47         |\n",
      "|    total_timesteps      | 14336      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01571112 |\n",
      "|    clip_fraction        | 0.162      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -28.3      |\n",
      "|    explained_variance   | 0.00251    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 53.6       |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0264    |\n",
      "|    std                  | 0.995      |\n",
      "|    value_loss           | 227        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.55e+03    |\n",
      "|    ep_rew_mean          | 6.09e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 303         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 53          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017224194 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -28.3       |\n",
      "|    explained_variance   | 0.000438    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 76.2        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0245     |\n",
      "|    std                  | 0.997       |\n",
      "|    value_loss           | 210         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.57e+03    |\n",
      "|    ep_rew_mean          | 6.27e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 302         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 60          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020517126 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -28.4       |\n",
      "|    explained_variance   | 0.000885    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 62.6        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0327     |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 191         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.57e+03    |\n",
      "|    ep_rew_mean          | 6.27e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 299         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 68          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016459642 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -28.4       |\n",
      "|    explained_variance   | 0.00065     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 52.7        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0229     |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 189         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x11f90ea9e10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "tmp_path = os.getcwd() + \"\\\\tensorboard_log\\\\sequencing\\\\\" + time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "new_logger = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "\n",
    "model = PPO(env=env, policy='MlpPolicy', verbose=1)\n",
    "model.set_logger(new_logger)\n",
    "model.learn(total_timesteps=20000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can review the training progress by looking at the tensorboard logs in the folder `tensorboard_log\\sequencing` in the current working directory. The following code will show the tensorboard logs in the notebook:\n",
    "\n",
    "``` bash\n",
    "tensorboard --logdir tensorboard_log\\sequencing\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example should only show the required implementation for an RL-agent for production control tasks. The routing control task can be implemented in a similar fashion. For more information on the implementation of the environment, please refer to the documentation of the abstract base classes in the [API reference](/API_reference/API_reference_0_overview.md) of `prodsys.control`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prodsys-9RcHADzK-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
